{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prioritized Experience Replay\n",
    "### 這一套算法重點就在我們 batch 抽樣的時候並不是隨機抽樣，而是按照 Memory 中的樣本優先級來抽，這能更有效地找到我們需要學習的樣本。    \n",
    "### 計算優先順序是使用TD-error，採用目標Q值與當前Q值的差值來表示。TD-error越大，優先級p越高。    \n",
    "### 每次抽樣都需要針對p對所有樣本排序, 這將會是一件非常消耗計算能力的事，所以用SumTree來做有效抽樣。\n",
    "參考：https://github.com/rlcode/per    \n",
    "    \n",
    "首先，先引入需要的函式庫，設定Episode為500，代表會跑500次遊戲。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "EPISODES = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 此為SumTree物件，用來存priority值。    \n",
    "要存儲的priority放在樹的葉結點上，也就是最下面一層。    \n",
    "[圖片來源：https://medium.com/@skywalker0803r/sumtree-algorithm-f150c0f7f0e4]\n",
    "![title](image/prioritizedReplay01.png)    \n",
    "    \n",
    "抽樣時，將root priority除以batch size，分成batch size區間(n=sum(p)/batch_size)，然後在每個區間隨機選取一個數(value)。    \n",
    "如果root node 的 priority 是42的話, 如果抽6個样本, 這時的區間擁有的 priority 可能是這樣：    \n",
    "`[0-7], [7-14], [14-21], [21-28], [28-35], [35-42]`    \n",
    "在區間`[21-28]`選到了24,就按照這個24從最頂上的42開始向下搜索,首先看到最頂上的42下面有兩個child nodes,拿著手中的24對比左邊的child 29,如果左邊的child比自己手中的值大,那就走左邊這條路,接著再對比29下面左邊那個點13這時手中的24比13大,那就走右邊那條路,並且將手中的值根據13修改一下,變成24–13=11,接著拿著11和16左下角的12比,結果12比11大,那就選12當作這次選到的priority,並且也選擇12對應的數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity #葉節點數量\n",
    "        self.tree = np.zeros(2 * capacity - 1) #總共節點的數量\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (np.abs(error) + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            while True:\n",
    "                s = random.uniform(a, b)\n",
    "                (idx, p, data) = self.tree.get(s)\n",
    "                if not isinstance(data, int):\n",
    "                    break\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate Q function using Neural Network\n",
    "# state is input and Q Value of each action is output of network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "與環境互動的Agent物件。    \n",
    "在`__init__`中做變數設定，如learning_rate, epsilon, batch_size等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and prioritized experience replay memory & target q network\n",
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.memory_size = 20000\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.explore_step = 5000\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / self.explore_step\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create prioritized replay memory using SumTree\n",
    "        self.memory = Memory(self.memory_size)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.model.apply(self.weights_init)\n",
    "        self.target_model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),\n",
    "                                    lr=self.learning_rate)\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model = torch.load('save_model/cartpole_dqn')\n",
    "\n",
    "    # weight xavier initialize\n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Linear') != -1:\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = torch.from_numpy(state)\n",
    "            state = Variable(state).float().cpu()\n",
    "            q_value = self.model(state)\n",
    "            _, action = torch.max(q_value, 1)\n",
    "            return int(action)\n",
    "\n",
    "    # save sample (error,<s,a,r,s'>) to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        target = self.model(Variable(torch.FloatTensor(state))).data\n",
    "        old_val = target[0][action]\n",
    "        target_val = self.target_model(Variable(torch.FloatTensor(next_state))).data\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.discount_factor * torch.max(target_val)\n",
    "\n",
    "        error = abs(old_val - target[0][action])\n",
    "\n",
    "        self.memory.add(error, (state, action, reward, next_state, done))\n",
    "\n",
    "    # pick samples from prioritized replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "        mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\n",
    "        mini_batch = np.array(mini_batch).transpose()\n",
    "\n",
    "        states = np.vstack(mini_batch[0])\n",
    "        actions = list(mini_batch[1])\n",
    "        rewards = list(mini_batch[2])\n",
    "        next_states = np.vstack(mini_batch[3])\n",
    "        dones = mini_batch[4]\n",
    "\n",
    "        # bool to binary\n",
    "        dones = dones.astype(int)\n",
    "\n",
    "        # Q function of current state\n",
    "        states = torch.Tensor(states)\n",
    "        states = Variable(states).float()\n",
    "        pred = self.model(states)\n",
    "\n",
    "        # one-hot encoding\n",
    "        a = torch.LongTensor(actions).view(-1, 1)\n",
    "\n",
    "        one_hot_action = torch.FloatTensor(self.batch_size, self.action_size).zero_()\n",
    "        one_hot_action.scatter_(1, a, 1)\n",
    "\n",
    "        pred = torch.sum(pred.mul(Variable(one_hot_action)), dim=1)\n",
    "\n",
    "        # Q function of next state\n",
    "        next_states = torch.Tensor(next_states)\n",
    "        next_states = Variable(next_states).float()\n",
    "        next_pred = self.target_model(next_states).data\n",
    "\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Q Learning: get maximum Q value at s' from target model\n",
    "        target = rewards + (1 - dones) * self.discount_factor * next_pred.max(1)[0]\n",
    "        target = Variable(target)\n",
    "\n",
    "        errors = torch.abs(pred - target).data.numpy()\n",
    "\n",
    "        # update priority\n",
    "        for i in range(self.batch_size):\n",
    "            idx = idxs[i]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # MSE Loss function\n",
    "        loss = (torch.FloatTensor(is_weights) * F.mse_loss(pred, target)).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        # and train\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "環境為CartPole-v1，會跑500個episode，如果最近十筆資料的score都大於490分，則會提前終止。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patty/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 16.0   memory length: 17   epsilon: 1.0\n",
      "episode: 1   score: 15.0   memory length: 33   epsilon: 1.0\n",
      "episode: 2   score: 23.0   memory length: 57   epsilon: 1.0\n",
      "episode: 3   score: 11.0   memory length: 69   epsilon: 1.0\n",
      "episode: 4   score: 13.0   memory length: 83   epsilon: 1.0\n",
      "episode: 5   score: 21.0   memory length: 105   epsilon: 1.0\n",
      "episode: 6   score: 35.0   memory length: 141   epsilon: 1.0\n",
      "episode: 7   score: 17.0   memory length: 159   epsilon: 1.0\n",
      "episode: 8   score: 9.0   memory length: 169   epsilon: 1.0\n",
      "episode: 9   score: 10.0   memory length: 180   epsilon: 1.0\n",
      "episode: 10   score: 10.0   memory length: 191   epsilon: 1.0\n",
      "episode: 11   score: 52.0   memory length: 244   epsilon: 1.0\n",
      "episode: 12   score: 53.0   memory length: 298   epsilon: 1.0\n",
      "episode: 13   score: 13.0   memory length: 312   epsilon: 1.0\n",
      "episode: 14   score: 16.0   memory length: 329   epsilon: 1.0\n",
      "episode: 15   score: 12.0   memory length: 342   epsilon: 1.0\n",
      "episode: 16   score: 49.0   memory length: 392   epsilon: 1.0\n",
      "episode: 17   score: 14.0   memory length: 407   epsilon: 1.0\n",
      "episode: 18   score: 8.0   memory length: 416   epsilon: 1.0\n",
      "episode: 19   score: 20.0   memory length: 437   epsilon: 1.0\n",
      "episode: 20   score: 14.0   memory length: 452   epsilon: 1.0\n",
      "episode: 21   score: 14.0   memory length: 467   epsilon: 1.0\n",
      "episode: 22   score: 22.0   memory length: 490   epsilon: 1.0\n",
      "episode: 23   score: 39.0   memory length: 530   epsilon: 1.0\n",
      "episode: 24   score: 22.0   memory length: 553   epsilon: 1.0\n",
      "episode: 25   score: 15.0   memory length: 569   epsilon: 1.0\n",
      "episode: 26   score: 21.0   memory length: 591   epsilon: 1.0\n",
      "episode: 27   score: 39.0   memory length: 631   epsilon: 1.0\n",
      "episode: 28   score: 45.0   memory length: 677   epsilon: 1.0\n",
      "episode: 29   score: 23.0   memory length: 701   epsilon: 1.0\n",
      "episode: 30   score: 10.0   memory length: 712   epsilon: 1.0\n",
      "episode: 31   score: 28.0   memory length: 741   epsilon: 1.0\n",
      "episode: 32   score: 16.0   memory length: 758   epsilon: 1.0\n",
      "episode: 33   score: 11.0   memory length: 770   epsilon: 1.0\n",
      "episode: 34   score: 9.0   memory length: 780   epsilon: 1.0\n",
      "episode: 35   score: 22.0   memory length: 803   epsilon: 1.0\n",
      "episode: 36   score: 13.0   memory length: 817   epsilon: 1.0\n",
      "episode: 37   score: 35.0   memory length: 853   epsilon: 1.0\n",
      "episode: 38   score: 16.0   memory length: 870   epsilon: 1.0\n",
      "episode: 39   score: 12.0   memory length: 883   epsilon: 1.0\n",
      "episode: 40   score: 16.0   memory length: 900   epsilon: 1.0\n",
      "episode: 41   score: 32.0   memory length: 933   epsilon: 1.0\n",
      "episode: 42   score: 74.0   memory length: 1008   epsilon: 0.9982179999999997\n",
      "episode: 43   score: 21.0   memory length: 1030   epsilon: 0.993861999999999\n",
      "episode: 44   score: 26.0   memory length: 1057   epsilon: 0.9885159999999982\n",
      "episode: 45   score: 22.0   memory length: 1080   epsilon: 0.9839619999999974\n",
      "episode: 46   score: 12.0   memory length: 1093   epsilon: 0.981387999999997\n",
      "episode: 47   score: 12.0   memory length: 1106   epsilon: 0.9788139999999966\n",
      "episode: 48   score: 15.0   memory length: 1122   epsilon: 0.9756459999999961\n",
      "episode: 49   score: 75.0   memory length: 1198   epsilon: 0.9605979999999937\n",
      "episode: 50   score: 9.0   memory length: 1208   epsilon: 0.9586179999999934\n",
      "episode: 51   score: 21.0   memory length: 1230   epsilon: 0.9542619999999927\n",
      "episode: 52   score: 12.0   memory length: 1243   epsilon: 0.9516879999999923\n",
      "episode: 53   score: 25.0   memory length: 1269   epsilon: 0.9465399999999915\n",
      "episode: 54   score: 12.0   memory length: 1282   epsilon: 0.9439659999999911\n",
      "episode: 55   score: 11.0   memory length: 1294   epsilon: 0.9415899999999907\n",
      "episode: 56   score: 36.0   memory length: 1331   epsilon: 0.9342639999999895\n",
      "episode: 57   score: 10.0   memory length: 1342   epsilon: 0.9320859999999892\n",
      "episode: 58   score: 15.0   memory length: 1358   epsilon: 0.9289179999999887\n",
      "episode: 59   score: 11.0   memory length: 1370   epsilon: 0.9265419999999883\n",
      "episode: 60   score: 26.0   memory length: 1397   epsilon: 0.9211959999999875\n",
      "episode: 61   score: 29.0   memory length: 1427   epsilon: 0.9152559999999865\n",
      "episode: 62   score: 31.0   memory length: 1459   epsilon: 0.9089199999999855\n",
      "episode: 63   score: 27.0   memory length: 1487   epsilon: 0.9033759999999846\n",
      "episode: 64   score: 15.0   memory length: 1503   epsilon: 0.9002079999999841\n",
      "episode: 65   score: 17.0   memory length: 1521   epsilon: 0.8966439999999836\n",
      "episode: 66   score: 14.0   memory length: 1536   epsilon: 0.8936739999999831\n",
      "episode: 67   score: 56.0   memory length: 1593   epsilon: 0.8823879999999813\n",
      "episode: 68   score: 44.0   memory length: 1638   epsilon: 0.8734779999999799\n",
      "episode: 69   score: 12.0   memory length: 1651   epsilon: 0.8709039999999795\n",
      "episode: 70   score: 41.0   memory length: 1693   epsilon: 0.8625879999999782\n",
      "episode: 71   score: 83.0   memory length: 1777   epsilon: 0.8459559999999755\n",
      "episode: 72   score: 43.0   memory length: 1821   epsilon: 0.8372439999999741\n",
      "episode: 73   score: 15.0   memory length: 1837   epsilon: 0.8340759999999736\n",
      "episode: 74   score: 25.0   memory length: 1863   epsilon: 0.8289279999999728\n",
      "episode: 75   score: 8.0   memory length: 1872   epsilon: 0.8271459999999725\n",
      "episode: 76   score: 18.0   memory length: 1891   epsilon: 0.8233839999999719\n",
      "episode: 77   score: 13.0   memory length: 1905   epsilon: 0.8206119999999715\n",
      "episode: 78   score: 16.0   memory length: 1922   epsilon: 0.8172459999999709\n",
      "episode: 79   score: 97.0   memory length: 2020   epsilon: 0.7978419999999679\n",
      "episode: 80   score: 10.0   memory length: 2031   epsilon: 0.7956639999999675\n",
      "episode: 81   score: 16.0   memory length: 2048   epsilon: 0.792297999999967\n",
      "episode: 82   score: 12.0   memory length: 2061   epsilon: 0.7897239999999666\n",
      "episode: 83   score: 9.0   memory length: 2071   epsilon: 0.7877439999999662\n",
      "episode: 84   score: 57.0   memory length: 2129   epsilon: 0.7762599999999644\n",
      "episode: 85   score: 19.0   memory length: 2149   epsilon: 0.7722999999999638\n",
      "episode: 86   score: 22.0   memory length: 2172   epsilon: 0.7677459999999631\n",
      "episode: 87   score: 45.0   memory length: 2218   epsilon: 0.7586379999999616\n",
      "episode: 88   score: 19.0   memory length: 2238   epsilon: 0.754677999999961\n",
      "episode: 89   score: 74.0   memory length: 2313   epsilon: 0.7398279999999586\n",
      "episode: 90   score: 20.0   memory length: 2334   epsilon: 0.735669999999958\n",
      "episode: 91   score: 13.0   memory length: 2348   epsilon: 0.7328979999999575\n",
      "episode: 92   score: 146.0   memory length: 2495   epsilon: 0.7037919999999529\n",
      "episode: 93   score: 19.0   memory length: 2515   epsilon: 0.6998319999999523\n",
      "episode: 94   score: 156.0   memory length: 2672   epsilon: 0.6687459999999473\n",
      "episode: 95   score: 85.0   memory length: 2758   epsilon: 0.6517179999999446\n",
      "episode: 96   score: 18.0   memory length: 2777   epsilon: 0.647955999999944\n",
      "episode: 97   score: 114.0   memory length: 2892   epsilon: 0.6251859999999404\n",
      "episode: 98   score: 193.0   memory length: 3086   epsilon: 0.5867739999999343\n",
      "episode: 99   score: 23.0   memory length: 3110   epsilon: 0.5820219999999335\n",
      "episode: 100   score: 115.0   memory length: 3226   epsilon: 0.5590539999999299\n",
      "episode: 101   score: 68.0   memory length: 3295   epsilon: 0.5453919999999277\n",
      "episode: 102   score: 49.0   memory length: 3345   epsilon: 0.5354919999999261\n",
      "episode: 103   score: 30.0   memory length: 3376   epsilon: 0.5293539999999252\n",
      "episode: 104   score: 131.0   memory length: 3508   epsilon: 0.503217999999921\n",
      "episode: 105   score: 133.0   memory length: 3642   epsilon: 0.47668599999992334\n",
      "episode: 106   score: 15.0   memory length: 3658   epsilon: 0.4735179999999237\n",
      "episode: 107   score: 120.0   memory length: 3779   epsilon: 0.44955999999992663\n",
      "episode: 108   score: 278.0   memory length: 4058   epsilon: 0.39431799999993333\n",
      "episode: 109   score: 99.0   memory length: 4158   epsilon: 0.37451799999993574\n",
      "episode: 110   score: 236.0   memory length: 4395   epsilon: 0.32759199999994143\n",
      "episode: 111   score: 212.0   memory length: 4608   epsilon: 0.28541799999994655\n",
      "episode: 112   score: 192.0   memory length: 4801   epsilon: 0.24720399999995077\n",
      "episode: 113   score: 272.0   memory length: 5074   epsilon: 0.19314999999994975\n",
      "episode: 114   score: 216.0   memory length: 5291   epsilon: 0.15018399999994894\n",
      "episode: 115   score: 285.0   memory length: 5577   epsilon: 0.09355599999994788\n",
      "episode: 116   score: 227.0   memory length: 5805   epsilon: 0.048411999999947525\n",
      "episode: 117   score: 214.0   memory length: 6020   epsilon: 0.009999999999947773\n",
      "episode: 118   score: 500.0   memory length: 6520   epsilon: 0.009999999999947773\n",
      "episode: 119   score: 379.0   memory length: 6900   epsilon: 0.009999999999947773\n",
      "episode: 120   score: 500.0   memory length: 7400   epsilon: 0.009999999999947773\n",
      "episode: 121   score: 253.0   memory length: 7654   epsilon: 0.009999999999947773\n",
      "episode: 122   score: 220.0   memory length: 7875   epsilon: 0.009999999999947773\n",
      "episode: 123   score: 222.0   memory length: 8098   epsilon: 0.009999999999947773\n",
      "episode: 124   score: 262.0   memory length: 8361   epsilon: 0.009999999999947773\n",
      "episode: 125   score: 214.0   memory length: 8576   epsilon: 0.009999999999947773\n",
      "episode: 126   score: 229.0   memory length: 8806   epsilon: 0.009999999999947773\n",
      "episode: 127   score: 337.0   memory length: 9144   epsilon: 0.009999999999947773\n",
      "episode: 128   score: 255.0   memory length: 9400   epsilon: 0.009999999999947773\n",
      "episode: 129   score: 204.0   memory length: 9605   epsilon: 0.009999999999947773\n",
      "episode: 130   score: 473.0   memory length: 10079   epsilon: 0.009999999999947773\n",
      "episode: 131   score: 259.0   memory length: 10339   epsilon: 0.009999999999947773\n",
      "episode: 132   score: 197.0   memory length: 10537   epsilon: 0.009999999999947773\n",
      "episode: 133   score: 189.0   memory length: 10727   epsilon: 0.009999999999947773\n",
      "episode: 134   score: 195.0   memory length: 10923   epsilon: 0.009999999999947773\n",
      "episode: 135   score: 224.0   memory length: 11148   epsilon: 0.009999999999947773\n",
      "episode: 136   score: 243.0   memory length: 11392   epsilon: 0.009999999999947773\n",
      "episode: 137   score: 254.0   memory length: 11647   epsilon: 0.009999999999947773\n",
      "episode: 138   score: 299.0   memory length: 11947   epsilon: 0.009999999999947773\n",
      "episode: 139   score: 209.0   memory length: 12157   epsilon: 0.009999999999947773\n",
      "episode: 140   score: 260.0   memory length: 12418   epsilon: 0.009999999999947773\n",
      "episode: 141   score: 317.0   memory length: 12736   epsilon: 0.009999999999947773\n",
      "episode: 142   score: 288.0   memory length: 13025   epsilon: 0.009999999999947773\n",
      "episode: 143   score: 257.0   memory length: 13283   epsilon: 0.009999999999947773\n",
      "episode: 144   score: 219.0   memory length: 13503   epsilon: 0.009999999999947773\n",
      "episode: 145   score: 251.0   memory length: 13755   epsilon: 0.009999999999947773\n",
      "episode: 146   score: 268.0   memory length: 14024   epsilon: 0.009999999999947773\n",
      "episode: 147   score: 270.0   memory length: 14295   epsilon: 0.009999999999947773\n",
      "episode: 148   score: 222.0   memory length: 14518   epsilon: 0.009999999999947773\n",
      "episode: 149   score: 277.0   memory length: 14796   epsilon: 0.009999999999947773\n",
      "episode: 150   score: 239.0   memory length: 15036   epsilon: 0.009999999999947773\n",
      "episode: 151   score: 259.0   memory length: 15296   epsilon: 0.009999999999947773\n",
      "episode: 152   score: 349.0   memory length: 15646   epsilon: 0.009999999999947773\n",
      "episode: 153   score: 384.0   memory length: 16031   epsilon: 0.009999999999947773\n",
      "episode: 154   score: 280.0   memory length: 16312   epsilon: 0.009999999999947773\n",
      "episode: 155   score: 301.0   memory length: 16614   epsilon: 0.009999999999947773\n",
      "episode: 156   score: 292.0   memory length: 16907   epsilon: 0.009999999999947773\n",
      "episode: 157   score: 335.0   memory length: 17243   epsilon: 0.009999999999947773\n",
      "episode: 158   score: 289.0   memory length: 17533   epsilon: 0.009999999999947773\n",
      "episode: 159   score: 317.0   memory length: 17851   epsilon: 0.009999999999947773\n",
      "episode: 160   score: 294.0   memory length: 18146   epsilon: 0.009999999999947773\n",
      "episode: 161   score: 360.0   memory length: 18507   epsilon: 0.009999999999947773\n",
      "episode: 162   score: 266.0   memory length: 18774   epsilon: 0.009999999999947773\n",
      "episode: 163   score: 369.0   memory length: 19144   epsilon: 0.009999999999947773\n",
      "episode: 164   score: 464.0   memory length: 19609   epsilon: 0.009999999999947773\n",
      "episode: 165   score: 340.0   memory length: 19950   epsilon: 0.009999999999947773\n",
      "episode: 166   score: 228.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 167   score: 280.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 168   score: 225.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 169   score: 251.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 170   score: 220.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 171   score: 256.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 172   score: 345.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 173   score: 272.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 174   score: 215.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 175   score: 175.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 176   score: 159.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 177   score: 140.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 178   score: 162.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 179   score: 145.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 180   score: 153.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 181   score: 158.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 182   score: 156.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 183   score: 170.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 184   score: 177.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 185   score: 158.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 186   score: 138.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 187   score: 143.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 188   score: 102.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 189   score: 106.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 190   score: 107.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 191   score: 103.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 192   score: 111.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 193   score: 119.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 194   score: 111.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 195   score: 122.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 196   score: 124.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 197   score: 135.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 198   score: 136.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 199   score: 143.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 200   score: 115.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 201   score: 128.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 202   score: 137.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 203   score: 154.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 204   score: 138.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 205   score: 125.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 206   score: 127.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 207   score: 130.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 208   score: 113.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 209   score: 111.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 210   score: 113.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 211   score: 118.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 212   score: 105.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 213   score: 115.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 214   score: 114.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 215   score: 119.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 216   score: 331.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 217   score: 144.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 218   score: 190.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 219   score: 166.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 220   score: 213.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 221   score: 360.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 222   score: 74.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 223   score: 28.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 224   score: 205.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 225   score: 209.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 226   score: 255.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 227   score: 171.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 228   score: 168.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 229   score: 68.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 230   score: 157.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 231   score: 255.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 232   score: 177.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 233   score: 157.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 234   score: 161.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 235   score: 166.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 236   score: 204.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 237   score: 167.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 238   score: 144.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 239   score: 140.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 240   score: 145.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 241   score: 187.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 242   score: 197.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 243   score: 227.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 244   score: 241.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 245   score: 314.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 246   score: 472.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 247   score: 371.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 248   score: 357.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 249   score: 325.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 250   score: 364.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 251   score: 413.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 252   score: 411.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 253   score: 355.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 254   score: 436.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 255   score: 418.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 256   score: 500.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 257   score: 500.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 258   score: 500.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 259   score: 500.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 260   score: 500.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 261   score: 500.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 262   score: 500.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 263   score: 500.0   memory length: 20000   epsilon: 0.009999999999947773\n",
      "episode: 264   score: 500.0   memory length: 20000   epsilon: 0.009999999999947773\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In case of CartPole-v1, maximum length of episode is 500\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "model = DQN(state_size, action_size)\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "scores, episodes = [], []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # get action for the current state and go one step in environment\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "        reward = reward if not done or score == 499 else -10\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        # every time step do the training\n",
    "        if agent.memory.tree.n_entries >= agent.train_start:\n",
    "            agent.train_model()\n",
    "\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            score = score if score == 500 else score + 10\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, scores, 'b')\n",
    "            pylab.savefig(\"./image/cartpole_dqn.png\")\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  agent.memory.tree.n_entries, \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 490\n",
    "            # stop training\n",
    "            if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                #torch.save(agent.model, \"./save_model/cartpole_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
